# -*- coding: utf-8 -*-
"""Dataset2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12fhPH1XOCYyobU1kGWEua_YvMmjbRFzh
"""
# Importing all required Libraries and setting sparkcontext

from bs4 import BeautifulSoup as bs
import requests
import json
import pandas as pd
import os
import shutil
from pyspark.sql.types import *
from pyspark import rdd
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import *
from pyspark.sql.functions import *
sc = SparkContext.getOrCreate()
spark = SparkSession.builder.appName('operation').getOrCreate()
print(sc)
print(spark)


# Reading csv file from AWS s3 bucket and printing as DataFrame

Links = pd.read_csv("https://diadataset.s3.amazonaws.com/Tripadvisor_Links.csv", header = None)
Links = list(Links[0])
Links.remove('Links')
print (pd.DataFrame(Links))
print("Total HTML Links for scrapping Data are:", len(list(Links)))


# Web-Scrapping using Beautiful Soup by HTML parser and saving it to Result[]
# Below script takes lot of time to parse 506 HTML links,
# So data is saved in JSON file for ready reference in AWS s3 bucket

# Result = []
# for Link in Links:
#     print (Link)
#     r = requests.get(Link)
#     soup = bs(r.content, "html.parser")
#     type(soup)
#     data_dict={"City":[],"Places":[],"Address":[],"Website":[],"Phone":[],"Reviews":[],"Rating (Out of 5)":[]}
#     title_elem = soup.find('h1',class_='ui_header h1')
#     print (title_elem.text)
#     data_dict["Places"]=(title_elem.text.strip())
#     address_elem = soup.find('div',class_='LjCWTZdN')
#     data_dict["Address"]=(address_elem.text.strip())
#     website_elem = soup.find('div',class_='_1ev9TQ-P')
#     data_dict["Website"]=(website_elem.a['href'])
#     phone_elem = soup.find('a',class_='_TF8HH3_')
#     data_dict["Phone"]=(phone_elem.text.strip())
#     Review_elem = soup.find('div',class_='_1NKYRldB')
#     data_dict["Reviews"]=(Review_elem.text.strip())
#     City_elem = soup.find('div',class_='eQSJNhO6')
#     City = City_elem.text.strip()
#     data_dict["City"]=(City.split(" ",7)[-1])
#     Rating_elem = soup.find('a',class_='_1d_R5B7y')
#     try:
#         data_dict["Rating (Out of 5)"]=(Rating_elem.text.strip())
#     except:
#         data_dict["Rating (Out of 5)"]=(Rating_elem)
#     Result.append(data_dict)
    
# final=pd.DataFrame().from_dict(Result)


# Read the JSON file from AWS s3 bucket and forming DataFrame

df = pd.read_json ('https://diadataset.s3.amazonaws.com/Attractions.json')
df.head(600)


# Using Pandas, cleaning data for further pre-processing

df1 = df["Reviews"].str.split(" ", n = 1 , expand = True)
df["Review"] = df1[0]
df["Text"] = df1[1]
df.rename(columns={"Rating (Out of 5)":"Ratings"}, inplace=True)
df["Review"] = df["Review"].str.replace(",","").astype(int)
df.drop(['Reviews' , 'Text'], axis='columns', inplace=True)
df_cln = pd.DataFrame(df.fillna({"Ratings": 0.0}))
df_cln


# Cleaned data in DataFrame is further converted in RDD using spark
# Using Map and lambda function took average of ratings

rdd = spark.createDataFrame(df_cln).rdd
rdd = rdd.map(lambda row: [str(c) for c in row])
rdd_new = rdd.map(lambda x: (x[0],x[1],x[5],x[6])).sortBy(lambda a: a[0])


# Result Output

rdd_new.take(600)


# Saving the output file to HDFS
rdd_new.saveAsTextFile("Dataset2_output")


